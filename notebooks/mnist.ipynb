{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = np.random.randn(output_dim, input_dim)\n",
    "    def forward_propagation(self, input_):\n",
    "        return self.W.dot(input_)\n",
    "    def backward_propagation(self, input_, gradient_wrt_output):\n",
    "        pass\n",
    "    def update_parameters(self, input_, gradient_wrt_output, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidLayer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward_propagation(self, input_):\n",
    "        #return np.apply_along_axis(lambda x : 1 / (1 + np.exp(-x)), axis=1, arr=input_)\n",
    "        return 1 / (1 + np.exp(-input_))\n",
    "    def backward_propagation(self, input_, gradient_wrt_output):\n",
    "        pass\n",
    "    def update_parameters(self, input_, gradient_wrt_output, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward_propagation(self, input_):\n",
    "        #return np.apply_along_axis( lambda x: np.exp(x) / np.sum(np.exp(x)), axis=1, arr=input_)\n",
    "        return np.exp(input_) / np.sum(np.exp(input_))\n",
    "    def backward_propagation(self, input_, gradient_wrt_output):\n",
    "        raise NotImplementedException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(indices, num_classes):\n",
    "    b = np.zeros( (indices.shape[0], num_classes) )\n",
    "    for i in range(0, b.shape[0]):\n",
    "        b[i, indices[i]-1 ] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"../data/mnist.pkl.gz\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt, yt = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape, yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in R^{b \\times p}$, where $b$ is the batch size and $p$ is the number of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "Xt_batch = Xt[0:1]\n",
    "yt_batch = one_hot( yt[0:1], num_classes=10)\n",
    "print Xt_batch.shape\n",
    "print yt_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [LinearLayer(784, 100), SigmoidLayer(), LinearLayer(100, 10), SoftmaxLayer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (784, 1)\n",
      "output shape: (100, 1)\n",
      "output shape: (100, 1)\n",
      "output shape: (10, 1)\n",
      "output shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = [Xt_batch.T]\n",
    "print \"output shape: %s\" % str(inputs[0].shape)\n",
    "for layer in layers:\n",
    "    inputs.append( layer.forward_propagation(inputs[-1]))\n",
    "    print \"output shape: %s\" % (str(inputs[-1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activations = inputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.18884540e-07],\n",
       "       [  2.57296012e-04],\n",
       "       [  4.18649281e-03],\n",
       "       [  1.07564875e-05],\n",
       "       [  9.95066677e-01],\n",
       "       [  4.64094912e-04],\n",
       "       [  1.37372161e-05],\n",
       "       [  4.78145991e-08],\n",
       "       [  7.78695972e-07],\n",
       "       [  1.07216160e-10]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in inputs[-1]:\n",
    "    assert sum(row) - 1.0 < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(input_, targets):\n",
    "    targets = targets.T\n",
    "    assert input_.shape[0] == targets.shape[0]\n",
    "    return -np.log(np.sum(targets * activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0049455319476775228"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(activations, yt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = activations\n",
    "t = yt_batch.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print y.shape\n",
    "print t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute $\\frac{dL}{dy}$, where $y = softmax(y')$. This is what the softmax layer will propagate back to the linear layer before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "dL_dy = -t / y\n",
    "print dL_dy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\frac{dL}{dy'}$, where $y' = Vh + c$. We will be backpropagating this to the previous layer (the sigmoid). Also, while we're inside this layer, we should also compute the derivatives needed for the parameter updates: $\\frac{dL}{dV}$ and $\\frac{dL}{dc}$ (these aren't backpropagated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "dL_dyprime = y - t\n",
    "print dL_dyprime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "h = inputs[2] # h = inputs[2] = sigm(Wx+b)\n",
    "dL_dV = np.dot(dL_dyprime, h.T)\n",
    "print dL_dV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dL_dc = dL_dyprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute $\\frac{dL}{dh}$, where $h = sigmoid(h') = sigmoid(Wx + b)$. We will be backpropagating $\\frac{dL}{dh}$ to the previous (and beginning) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = layers[2].W\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dh = np.dot(V.T, dL_dyprime)\n",
    "dL_dh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, last time we do this. We have $\\frac{dL}{dh}$ from the following layer and all we need to do is compute $\\frac{dL}{dh'}$. From this, we can compute the parameter gradients $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ and we'll be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_prime = inputs[1] # h' = inputs[1] = Wx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dL_dh = dL_dh * sigmoid(h') * (1 - sigmoid(h'))\n",
    "dL_dhprime = dL_dh * h * (1 - h)\n",
    "dL_dhprime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dW = np.dot(dL_dhprime, inputs[0].T)\n",
    "dL_dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_db = dL_dhprime\n",
    "dL_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try this by implementing backward_propagation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OneLayerMLP(object):\n",
    "    def __init__(self, num_input_units, num_hidden_units, num_output_units):\n",
    "        self.W = np.random.randn(num_hidden_units, num_input_units)\n",
    "        self.V = np.random.randn(num_output_units, num_hidden_units)\n",
    "        self.b = np.random.randn(num_hidden_units, 1)\n",
    "        self.c = np.random.randn(num_output_units, 1)\n",
    "        self.num_input_units = num_input_units\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_output_units = num_output_units\n",
    "    def loss(self, activation, target):\n",
    "        assert activation.shape[0] == target.shape[0]\n",
    "        return -np.log(np.sum(target * activation))\n",
    "    def classify(self, x):\n",
    "        return self.forward(x)[-1]\n",
    "    def forward(self, x):\n",
    "        assert x.shape == (self.num_input_units, 1)\n",
    "        hprime = np.dot(self.W, x) + self.b\n",
    "        h = sigmoid(hprime)\n",
    "        yprime = np.dot(self.V, h) + self.c\n",
    "        y = softmax(yprime)\n",
    "        assert y.shape == (self.num_output_units, 1)\n",
    "        return [x, hprime, h, yprime, h, y]\n",
    "    def backprop(self, inputs, target, learning_rate):\n",
    "        x = inputs[0]\n",
    "        y = inputs[-1]\n",
    "        dL_dy = -t / y\n",
    "        dL_dyprime = y - t\n",
    "        h = inputs[2] # h = inputs[2] = sigm(Wx+b)\n",
    "        dL_dV = np.dot(dL_dyprime, h.T)\n",
    "        dL_dc = dL_dyprime\n",
    "        dL_dh = np.dot(self.V.T, dL_dyprime)\n",
    "        h_prime = inputs[1]\n",
    "        dL_dhprime = dL_dh * h * (1 - h)\n",
    "        dL_dW = np.dot(dL_dhprime, x.T)\n",
    "        dL_db = dL_dhprime\n",
    "        # test\n",
    "        assert dL_dW.shape == self.W.shape\n",
    "        assert dL_db.shape == self.b.shape\n",
    "        assert dL_dV.shape == self.V.shape\n",
    "        assert dL_dc.shape == self.c.shape\n",
    "        # ok do the parameter updates\n",
    "        self.W = self.W - learning_rate*dL_dW\n",
    "        self.b = self.b - learning_rate*dL_db\n",
    "        self.V = self.V - learning_rate*dL_dV\n",
    "        self.c = self.c - learning_rate*dL_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "mlp = OneLayerMLP(num_input_units=784, num_hidden_units=100, num_output_units=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-520-332d689e0b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mthis_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"this loss: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-507-39ac80e6bc09>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, inputs, target, learning_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mh_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdL_dhprime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_dh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdL_dW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dhprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdL_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_dhprime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# ok do the parameter updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 10):\n",
    "    this_losses = []\n",
    "    for i in range(0, Xt.shape[0]):\n",
    "        Xt_batch = Xt[i:i+1].T\n",
    "        yt_batch = one_hot( yt[i:i+1], num_classes=10 ).T\n",
    "        outputs = mlp.forward(Xt_batch)\n",
    "        this_losses.append(mlp.loss(outputs[-1], yt_batch))\n",
    "        mlp.backprop(outputs, yt_batch, 0.01)\n",
    "    print \"this loss: %d\" % (np.mean(this_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt[0:10].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(mlp.classify(Xt[2:2+1].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.22259175e-10],\n",
       "       [  1.12633509e-07],\n",
       "       [  3.88029628e-08],\n",
       "       [  7.61745343e-13],\n",
       "       [  2.76263221e-06],\n",
       "       [  5.38590484e-11],\n",
       "       [  9.04481362e-10],\n",
       "       [  1.54028889e-01],\n",
       "       [  1.67873714e-04],\n",
       "       [  8.45800322e-01]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = mlp.forward(Xt_batch.T, yt_batch.T)\n",
    "outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18877156],\n",
       "       [ 1.47475715],\n",
       "       [ 2.04789582],\n",
       "       [ 0.07034556],\n",
       "       [-0.17854169],\n",
       "       [ 1.01649925],\n",
       "       [-1.09767397],\n",
       "       [-1.05564543],\n",
       "       [ 2.73785476],\n",
       "       [ 2.18934815]])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.backprop(outs, yt_batch, 0.1)\n",
    "mlp.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
