{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is currently very messy! Big credit goes to [Olexa](https://github.com/obilaniu/IFT6266/blob/master/HW/HW1/HW1.ipynb) as I had some issues doing the backprop myself. He goes through the derivations step by step and they are very helpful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.umath_tests import matrix_multiply\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"../data/mnist.pkl.gz\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt, yt = train_set\n",
    "Xv, yv = valid_set\n",
    "Xtest, ytest = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape, yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0049455319476775228"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(activations, yt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(indices, num_classes):\n",
    "    b = np.zeros( (indices.shape[0], num_classes) )\n",
    "    for i in range(0, b.shape[0]):\n",
    "        b[i, indices[i] ] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(input_, targets):\n",
    "    targets = targets.T\n",
    "    assert input_.shape[0] == targets.shape[0]\n",
    "    return -np.log(np.sum(targets * activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OneLayerMLP(object):\n",
    "    def __init__(self, num_input_units, num_hidden_units, num_output_units):\n",
    "        self.W = np.random.normal(0, 1, (num_hidden_units, num_input_units))\n",
    "        self.V = np.random.normal(0, 1, (num_output_units, num_hidden_units))\n",
    "        self.b = np.random.normal(0, 1, (num_hidden_units, 1))\n",
    "        self.c = np.random.normal(0, 1, (num_output_units, 1))\n",
    "        self.num_input_units = num_input_units\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_output_units = num_output_units\n",
    "    def loss(self, activation, target):\n",
    "        assert activation.shape[0] == target.shape[0]\n",
    "        return -np.log(np.sum(target * activation))\n",
    "    def classify(self, x):\n",
    "        return self.forward(x)[-1]\n",
    "    def forward(self, x, debug=False):\n",
    "        assert x.shape[0] == self.num_input_units\n",
    "        hprime = np.dot(self.W, x) + self.b\n",
    "        h = sigmoid(hprime)\n",
    "        yprime = np.dot(self.V, h) + self.c\n",
    "        y = softmax(yprime)\n",
    "        assert y.shape[0] == self.num_output_units\n",
    "        if debug:\n",
    "            print x.shape\n",
    "            print hprime.shape\n",
    "            print h.shape\n",
    "            print yprime.shape\n",
    "            print y.shape\n",
    "        return [x, hprime, h, yprime, y]\n",
    "    def backprop(self, inputs, target, debug=False):\n",
    "        x = inputs[0]\n",
    "        y = inputs[-1]\n",
    "        t = target\n",
    "        dL_dy = -t / y\n",
    "        dL_dyprime = y - t\n",
    "        h = inputs[2] # h = inputs[2] = sigm(Wx+b)\n",
    "        dL_dV = np.dot(dL_dyprime, h.T)\n",
    "        dL_dc = dL_dyprime\n",
    "        dL_dh = np.dot(self.V.T, dL_dyprime)\n",
    "        h_prime = inputs[1]\n",
    "        dL_dhprime = dL_dh * h * (1 - h)\n",
    "        dL_dW = np.dot(dL_dhprime, x.T)\n",
    "        dL_db = dL_dhprime\n",
    "        #assert dL_dW.shape == self.W.shape\n",
    "        #assert dL_db.shape == self.b.shape\n",
    "        #assert dL_dV.shape == self.V.shape\n",
    "        #assert dL_dc.shape == self.c.shape\n",
    "        return dL_dW, dL_db, dL_dV, dL_dc\n",
    "    def update(self, grads, learning_rate):\n",
    "        dL_dW, dL_db, dL_dV, dL_dc = grads\n",
    "        self.W = self.W - learning_rate*dL_dW\n",
    "        self.b = self.b - learning_rate*dL_db\n",
    "        self.V = self.V - learning_rate*dL_dV\n",
    "        self.c = self.c - learning_rate*dL_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss, train_acc, valid_acc = 0.718387, 0.876900, 0.883900\n",
      "loss, train_acc, valid_acc = 0.350320, 0.904360, 0.909500\n",
      "loss, train_acc, valid_acc = 0.286921, 0.917900, 0.919300\n",
      "loss, train_acc, valid_acc = 0.250477, 0.926180, 0.925800\n",
      "loss, train_acc, valid_acc = 0.224548, 0.932940, 0.929700\n",
      "loss, train_acc, valid_acc = 0.204418, 0.938160, 0.932100\n",
      "loss, train_acc, valid_acc = 0.188110, 0.942820, 0.934600\n",
      "loss, train_acc, valid_acc = 0.174434, 0.947040, 0.938900\n",
      "loss, train_acc, valid_acc = 0.162622, 0.950420, 0.941200\n",
      "loss, train_acc, valid_acc = 0.152259, 0.953720, 0.941800\n",
      "loss, train_acc, valid_acc = 0.143038, 0.956420, 0.943600\n",
      "loss, train_acc, valid_acc = 0.134716, 0.958920, 0.945100\n",
      "loss, train_acc, valid_acc = 0.127163, 0.961900, 0.946700\n",
      "loss, train_acc, valid_acc = 0.120295, 0.964080, 0.947400\n",
      "loss, train_acc, valid_acc = 0.114032, 0.966180, 0.948400\n",
      "loss, train_acc, valid_acc = 0.108287, 0.968100, 0.948700\n",
      "loss, train_acc, valid_acc = 0.102976, 0.969780, 0.949400\n",
      "loss, train_acc, valid_acc = 0.098025, 0.971200, 0.950000\n",
      "loss, train_acc, valid_acc = 0.093385, 0.972580, 0.951000\n",
      "loss, train_acc, valid_acc = 0.089025, 0.973920, 0.951200\n",
      "loss, train_acc, valid_acc = 0.084929, 0.975180, 0.951400\n",
      "loss, train_acc, valid_acc = 0.081082, 0.976520, 0.952000\n",
      "loss, train_acc, valid_acc = 0.077469, 0.977740, 0.953400\n",
      "loss, train_acc, valid_acc = 0.074072, 0.979020, 0.953200\n",
      "loss, train_acc, valid_acc = 0.070877, 0.980140, 0.953600\n",
      "loss, train_acc, valid_acc = 0.067874, 0.980920, 0.954600\n",
      "loss, train_acc, valid_acc = 0.065048, 0.981900, 0.954700\n",
      "loss, train_acc, valid_acc = 0.062381, 0.982840, 0.954700\n",
      "loss, train_acc, valid_acc = 0.059854, 0.983820, 0.955400\n",
      "loss, train_acc, valid_acc = 0.057455, 0.984740, 0.955600\n",
      "loss, train_acc, valid_acc = 0.055181, 0.985740, 0.955800\n",
      "loss, train_acc, valid_acc = 0.053026, 0.986520, 0.956000\n",
      "loss, train_acc, valid_acc = 0.050981, 0.987220, 0.956100\n",
      "loss, train_acc, valid_acc = 0.049042, 0.987840, 0.956400\n",
      "loss, train_acc, valid_acc = 0.047204, 0.988600, 0.956800\n",
      "loss, train_acc, valid_acc = 0.045458, 0.989040, 0.956600\n",
      "loss, train_acc, valid_acc = 0.043795, 0.989620, 0.956500\n",
      "loss, train_acc, valid_acc = 0.042206, 0.990220, 0.956900\n",
      "loss, train_acc, valid_acc = 0.040685, 0.990820, 0.956800\n",
      "loss, train_acc, valid_acc = 0.039222, 0.991500, 0.957400\n",
      "loss, train_acc, valid_acc = 0.037812, 0.991840, 0.957700\n",
      "loss, train_acc, valid_acc = 0.036456, 0.992180, 0.957900\n",
      "loss, train_acc, valid_acc = 0.035157, 0.992680, 0.957700\n",
      "loss, train_acc, valid_acc = 0.033915, 0.993020, 0.957900\n",
      "loss, train_acc, valid_acc = 0.032727, 0.993260, 0.958100\n",
      "loss, train_acc, valid_acc = 0.031590, 0.993640, 0.958100\n",
      "loss, train_acc, valid_acc = 0.030500, 0.993860, 0.958400\n",
      "loss, train_acc, valid_acc = 0.029454, 0.994240, 0.958500\n",
      "loss, train_acc, valid_acc = 0.028448, 0.994520, 0.958400\n",
      "loss, train_acc, valid_acc = 0.027480, 0.994720, 0.958400\n",
      "loss, train_acc, valid_acc = 0.026547, 0.994940, 0.958200\n",
      "loss, train_acc, valid_acc = 0.025648, 0.995180, 0.958300\n",
      "loss, train_acc, valid_acc = 0.024780, 0.995360, 0.958400\n",
      "loss, train_acc, valid_acc = 0.023945, 0.995540, 0.958300\n",
      "loss, train_acc, valid_acc = 0.023141, 0.995840, 0.958700\n",
      "loss, train_acc, valid_acc = 0.022368, 0.996040, 0.958800\n",
      "loss, train_acc, valid_acc = 0.021627, 0.996200, 0.958700\n",
      "loss, train_acc, valid_acc = 0.020915, 0.996380, 0.958800\n",
      "loss, train_acc, valid_acc = 0.020231, 0.996520, 0.958600\n",
      "loss, train_acc, valid_acc = 0.019574, 0.996800, 0.958900\n",
      "loss, train_acc, valid_acc = 0.018942, 0.996960, 0.958900\n",
      "loss, train_acc, valid_acc = 0.018334, 0.997060, 0.958700\n",
      "loss, train_acc, valid_acc = 0.017749, 0.997140, 0.958600\n",
      "loss, train_acc, valid_acc = 0.017186, 0.997300, 0.958800\n",
      "loss, train_acc, valid_acc = 0.016643, 0.997400, 0.958600\n",
      "loss, train_acc, valid_acc = 0.016121, 0.997500, 0.958600\n",
      "loss, train_acc, valid_acc = 0.015618, 0.997620, 0.958700\n",
      "loss, train_acc, valid_acc = 0.015135, 0.997680, 0.958900\n",
      "loss, train_acc, valid_acc = 0.014672, 0.997780, 0.959200\n",
      "loss, train_acc, valid_acc = 0.014227, 0.997860, 0.959100\n",
      "loss, train_acc, valid_acc = 0.013801, 0.997960, 0.959000\n",
      "loss, train_acc, valid_acc = 0.013393, 0.998040, 0.959000\n",
      "loss, train_acc, valid_acc = 0.013003, 0.998140, 0.959100\n",
      "loss, train_acc, valid_acc = 0.012630, 0.998140, 0.959300\n",
      "loss, train_acc, valid_acc = 0.012273, 0.998240, 0.959800\n",
      "loss, train_acc, valid_acc = 0.011931, 0.998280, 0.959800\n",
      "loss, train_acc, valid_acc = 0.011603, 0.998380, 0.960000\n",
      "loss, train_acc, valid_acc = 0.011289, 0.998500, 0.960100\n",
      "loss, train_acc, valid_acc = 0.010986, 0.998520, 0.960300\n",
      "loss, train_acc, valid_acc = 0.010695, 0.998580, 0.960000\n",
      "loss, train_acc, valid_acc = 0.010414, 0.998680, 0.959800\n",
      "loss, train_acc, valid_acc = 0.010143, 0.998880, 0.959700\n",
      "loss, train_acc, valid_acc = 0.009881, 0.998900, 0.959600\n",
      "loss, train_acc, valid_acc = 0.009628, 0.998960, 0.959400\n",
      "loss, train_acc, valid_acc = 0.009385, 0.999040, 0.959500\n",
      "loss, train_acc, valid_acc = 0.009152, 0.999060, 0.959400\n",
      "loss, train_acc, valid_acc = 0.008929, 0.999080, 0.959600\n",
      "loss, train_acc, valid_acc = 0.008714, 0.999140, 0.959700\n",
      "loss, train_acc, valid_acc = 0.008508, 0.999180, 0.959900\n",
      "loss, train_acc, valid_acc = 0.008310, 0.999260, 0.960000\n",
      "loss, train_acc, valid_acc = 0.008120, 0.999300, 0.960100\n",
      "loss, train_acc, valid_acc = 0.007937, 0.999320, 0.960100\n",
      "loss, train_acc, valid_acc = 0.007760, 0.999360, 0.960100\n",
      "loss, train_acc, valid_acc = 0.007590, 0.999380, 0.960000\n",
      "loss, train_acc, valid_acc = 0.007426, 0.999420, 0.959900\n",
      "loss, train_acc, valid_acc = 0.007268, 0.999440, 0.959900\n",
      "loss, train_acc, valid_acc = 0.007115, 0.999440, 0.959800\n",
      "loss, train_acc, valid_acc = 0.006967, 0.999460, 0.959800\n",
      "loss, train_acc, valid_acc = 0.006824, 0.999480, 0.959800\n",
      "loss, train_acc, valid_acc = 0.006685, 0.999540, 0.959800\n",
      "loss, train_acc, valid_acc = 0.006551, 0.999540, 0.959800\n",
      "loss, train_acc, valid_acc = 0.006421, 0.999560, 0.960000\n",
      "loss, train_acc, valid_acc = 0.006294, 0.999580, 0.959900\n",
      "loss, train_acc, valid_acc = 0.006172, 0.999580, 0.959900\n",
      "loss, train_acc, valid_acc = 0.006053, 0.999580, 0.960100\n",
      "loss, train_acc, valid_acc = 0.005938, 0.999580, 0.960100\n",
      "loss, train_acc, valid_acc = 0.005826, 0.999620, 0.960000\n",
      "loss, train_acc, valid_acc = 0.005717, 0.999640, 0.960000\n",
      "loss, train_acc, valid_acc = 0.005611, 0.999640, 0.960000\n",
      "loss, train_acc, valid_acc = 0.005508, 0.999660, 0.960000\n",
      "loss, train_acc, valid_acc = 0.005408, 0.999700, 0.960200\n",
      "loss, train_acc, valid_acc = 0.005310, 0.999700, 0.960100\n",
      "loss, train_acc, valid_acc = 0.005215, 0.999740, 0.960100\n",
      "loss, train_acc, valid_acc = 0.005122, 0.999740, 0.960100\n",
      "loss, train_acc, valid_acc = 0.005032, 0.999760, 0.960100\n",
      "loss, train_acc, valid_acc = 0.004943, 0.999760, 0.960200\n",
      "loss, train_acc, valid_acc = 0.004857, 0.999780, 0.960200\n",
      "loss, train_acc, valid_acc = 0.004772, 0.999780, 0.960200\n",
      "loss, train_acc, valid_acc = 0.004691, 0.999780, 0.960200\n",
      "loss, train_acc, valid_acc = 0.004612, 0.999800, 0.960300\n",
      "loss, train_acc, valid_acc = 0.004535, 0.999800, 0.960300\n",
      "loss, train_acc, valid_acc = 0.004460, 0.999820, 0.960300\n",
      "loss, train_acc, valid_acc = 0.004388, 0.999820, 0.960400\n",
      "loss, train_acc, valid_acc = 0.004317, 0.999840, 0.960300\n",
      "loss, train_acc, valid_acc = 0.004249, 0.999840, 0.960400\n",
      "loss, train_acc, valid_acc = 0.004182, 0.999860, 0.960400\n",
      "loss, train_acc, valid_acc = 0.004118, 0.999860, 0.960500\n",
      "loss, train_acc, valid_acc = 0.004054, 0.999880, 0.960500\n",
      "loss, train_acc, valid_acc = 0.003993, 0.999880, 0.960500\n",
      "loss, train_acc, valid_acc = 0.003933, 0.999880, 0.960500\n",
      "loss, train_acc, valid_acc = 0.003874, 0.999880, 0.960600\n",
      "loss, train_acc, valid_acc = 0.003817, 0.999880, 0.960600\n",
      "loss, train_acc, valid_acc = 0.003761, 0.999880, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003707, 0.999880, 0.960400\n",
      "loss, train_acc, valid_acc = 0.003654, 0.999880, 0.960400\n",
      "loss, train_acc, valid_acc = 0.003602, 0.999880, 0.960500\n",
      "loss, train_acc, valid_acc = 0.003552, 0.999880, 0.960500\n",
      "loss, train_acc, valid_acc = 0.003502, 0.999880, 0.960400\n",
      "loss, train_acc, valid_acc = 0.003454, 0.999880, 0.960400\n",
      "loss, train_acc, valid_acc = 0.003406, 0.999880, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003360, 0.999880, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003315, 0.999900, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003271, 0.999900, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003228, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003185, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003144, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.003104, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.003064, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.003025, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002987, 0.999920, 0.960200\n",
      "loss, train_acc, valid_acc = 0.002950, 0.999920, 0.960200\n",
      "loss, train_acc, valid_acc = 0.002914, 0.999920, 0.960200\n",
      "loss, train_acc, valid_acc = 0.002878, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.002843, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.002809, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.002776, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.002743, 0.999920, 0.960200\n",
      "loss, train_acc, valid_acc = 0.002711, 0.999920, 0.960200\n",
      "loss, train_acc, valid_acc = 0.002679, 0.999920, 0.960300\n",
      "loss, train_acc, valid_acc = 0.002648, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002618, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002588, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002559, 0.999920, 0.960500\n",
      "loss, train_acc, valid_acc = 0.002530, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002502, 0.999920, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002475, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002448, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002421, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002395, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002370, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002345, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002320, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002296, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002272, 0.999940, 0.960500\n",
      "loss, train_acc, valid_acc = 0.002249, 0.999940, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002226, 0.999960, 0.960500\n",
      "loss, train_acc, valid_acc = 0.002204, 0.999960, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002182, 0.999960, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002160, 0.999960, 0.960400\n",
      "loss, train_acc, valid_acc = 0.002139, 0.999980, 0.960500\n",
      "loss, train_acc, valid_acc = 0.002118, 0.999980, 0.960500\n",
      "loss, train_acc, valid_acc = 0.002097, 0.999980, 0.960600\n",
      "loss, train_acc, valid_acc = 0.002077, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.002057, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.002038, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.002019, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.002000, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.001981, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.001963, 0.999980, 0.960900\n",
      "loss, train_acc, valid_acc = 0.001945, 0.999980, 0.961000\n",
      "loss, train_acc, valid_acc = 0.001927, 0.999980, 0.961100\n",
      "loss, train_acc, valid_acc = 0.001910, 0.999980, 0.961100\n",
      "loss, train_acc, valid_acc = 0.001893, 0.999980, 0.961100\n",
      "loss, train_acc, valid_acc = 0.001876, 0.999980, 0.961200\n",
      "loss, train_acc, valid_acc = 0.001860, 0.999980, 0.961200\n",
      "loss, train_acc, valid_acc = 0.001843, 0.999980, 0.961300\n",
      "loss, train_acc, valid_acc = 0.001827, 0.999980, 0.961300\n",
      "loss, train_acc, valid_acc = 0.001812, 0.999980, 0.961300\n",
      "loss, train_acc, valid_acc = 0.001796, 0.999980, 0.961300\n",
      "loss, train_acc, valid_acc = 0.001781, 0.999980, 0.961300\n"
     ]
    }
   ],
   "source": [
    "mlp = OneLayerMLP(num_input_units=784, num_hidden_units=100, num_output_units=10)\n",
    "for epoch in range(0, 200):\n",
    "    this_losses = []\n",
    "    for i in range(0, Xt.shape[0]):\n",
    "        Xt_batch = Xt[i:i+1].T\n",
    "        yt_batch = one_hot( yt[i:i+1], num_classes=10 ).T\n",
    "        outputs = mlp.forward(Xt_batch)\n",
    "        this_losses.append(mlp.loss(outputs[-1], yt_batch))\n",
    "        grads = mlp.backprop(outputs, yt_batch)\n",
    "        mlp.update(grads, learning_rate=0.01)\n",
    "        #break\n",
    "    this_epoch_loss = (np.mean(this_losses))\n",
    "    preds = np.argmax(mlp.classify(Xt.T).T, axis=1)\n",
    "    this_epoch_train_acc = np.sum(yt == preds) * 1.0 / len(preds)\n",
    "    preds = np.argmax(mlp.classify(Xv.T).T, axis=1)\n",
    "    this_epoch_valid_acc = np.sum(yv == preds) * 1.0 / len(preds)\n",
    "    print \"loss, train_acc, valid_acc = %f, %f, %f\" % (this_epoch_loss, this_epoch_train_acc, this_epoch_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95569999999999999"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = np.argmax(mlp.classify(Xtest.T).T, axis=1)\n",
    "np.sum(ytest == test_preds) * 1.0 / len(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(mlp.classify(Xt.T).T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88246000000000002"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(yt == preds) * 1.0 / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.39979656e-07,   9.23638597e-01],\n",
       "       [  2.20138583e-07,   5.04057111e-09],\n",
       "       [  1.91417110e-07,   1.35390203e-05],\n",
       "       [  2.56108142e-04,   2.88408791e-04],\n",
       "       [  1.24316110e-08,   4.94803472e-07],\n",
       "       [  6.71661825e-02,   8.34923910e-03],\n",
       "       [  4.85142025e-07,   1.97681506e-05],\n",
       "       [  1.10287884e-07,   1.77744941e-04],\n",
       "       [  2.40798434e-06,   4.80289883e-05],\n",
       "       [  1.22971522e-06,   3.69864278e-05]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(Xt[0:2].T)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Using the finite differences method, implement a numerical version of the code that computes the gradients, and compare the analytical gradients and the numerical gradients using a toy example (e.g. a random pair of feature and target vectors for an MLP with 10 input dimensions, 5 hidden units and 2 categories). A mismatch between the two indicates an error in the implementation of the analytical gradients. The numpy.testing.assert_allclose method will be handy for that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, first let's write a function that will give us a random instance pair from MNIST with 10 features and 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_small_instance(Xt, yt, num_classes=10):\n",
    "    idx = np.random.randint(0, Xt.shape[0])\n",
    "    rand_x = Xt[idx:idx+1]\n",
    "    rand_y = yt[idx:idx+1]\n",
    "    if rand_y[0] in [0,1,2,3,4]:\n",
    "        rand_y[0] = 0\n",
    "    else:\n",
    "        rand_y[0] = 1\n",
    "    rand_y = one_hot(rand_y, num_classes=2).T\n",
    "    #print rand_x\n",
    "    idx = np.random.randint(0, Xt.shape[1])\n",
    "    rand_x = rand_x[:,idx:idx+10].T\n",
    "    return rand_x, rand_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through all the parameters of the small MLP, collect the gradients calculated using finite differences, then get the actual backprop gradients, and run a comparison using `np.testing.assert_allclose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_small, y_small = get_random_small_instance(Xt, yt)\n",
    "small_mlp = OneLayerMLP(num_input_units=x_small.shape[0], num_hidden_units=5, num_output_units=y_small.shape[0])\n",
    "delta = 1e-6\n",
    "all_grads = []\n",
    "how_many = 0\n",
    "for matrix in [small_mlp.W, small_mlp.b, small_mlp.V, small_mlp.c]:\n",
    "    how_many += (matrix.shape[0]*matrix.shape[1])\n",
    "    for i in range(0, matrix.shape[0]):\n",
    "        for j in range(0, matrix.shape[1]):\n",
    "            this_value_original = matrix[i,j]\n",
    "            # set theta = theta - delta, then calculate cost\n",
    "            matrix[i,j] = matrix[i,j] - delta\n",
    "            this_cost_minus = small_mlp.loss( small_mlp.forward(x_small)[-1], y_small )\n",
    "            # set theta = theta + delta, then calculate cost\n",
    "            matrix[i,j] = this_value_original + delta\n",
    "            this_cost_plus = small_mlp.loss( small_mlp.forward(x_small)[-1], y_small )\n",
    "            all_grads.append( ((this_cost_plus - this_cost_minus) / (2*delta)) )\n",
    "            matrix[i,j] = this_value_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(all_grads) == how_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_outputs = small_mlp.forward(x_small)\n",
    "p1, p2, p3, p4 = small_mlp.backprop(this_outputs, y_small)\n",
    "actual_all_grads = []\n",
    "actual_all_grads += p1.flatten().tolist()\n",
    "actual_all_grads += p2.flatten().tolist()\n",
    "actual_all_grads += p3.flatten().tolist()\n",
    "actual_all_grads += p4.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(all_grads) == len(actual_all_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(all_grads, actual_all_grads, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
