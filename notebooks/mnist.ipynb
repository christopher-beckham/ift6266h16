{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is currently very messy! Big credit goes to [Olexa](https://github.com/obilaniu/IFT6266/blob/master/HW/HW1/HW1.ipynb) as I had some issues doing the backprop myself. He goes through the derivations step by step and they are very helpful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = np.random.randn(output_dim, input_dim)\n",
    "    def forward_propagation(self, input_):\n",
    "        return self.W.dot(input_)\n",
    "    def backward_propagation(self, input_, gradient_wrt_output):\n",
    "        pass\n",
    "    def update_parameters(self, input_, gradient_wrt_output, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidLayer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward_propagation(self, input_):\n",
    "        #return np.apply_along_axis(lambda x : 1 / (1 + np.exp(-x)), axis=1, arr=input_)\n",
    "        return 1 / (1 + np.exp(-input_))\n",
    "    def backward_propagation(self, input_, gradient_wrt_output):\n",
    "        pass\n",
    "    def update_parameters(self, input_, gradient_wrt_output, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward_propagation(self, input_):\n",
    "        #return np.apply_along_axis( lambda x: np.exp(x) / np.sum(np.exp(x)), axis=1, arr=input_)\n",
    "        return np.exp(input_) / np.sum(np.exp(input_))\n",
    "    def backward_propagation(self, input_, gradient_wrt_output):\n",
    "        raise NotImplementedException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(indices, num_classes):\n",
    "    b = np.zeros( (indices.shape[0], num_classes) )\n",
    "    for i in range(0, b.shape[0]):\n",
    "        b[i, indices[i] ] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"../data/mnist.pkl.gz\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt, yt = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape, yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in R^{b \\times p}$, where $b$ is the batch size and $p$ is the number of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "Xt_batch = Xt[0:1]\n",
    "yt_batch = one_hot( yt[0:1], num_classes=10)\n",
    "print Xt_batch.shape\n",
    "print yt_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [LinearLayer(784, 100), SigmoidLayer(), LinearLayer(100, 10), SoftmaxLayer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (784, 1)\n",
      "output shape: (100, 1)\n",
      "output shape: (100, 1)\n",
      "output shape: (10, 1)\n",
      "output shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = [Xt_batch.T]\n",
    "print \"output shape: %s\" % str(inputs[0].shape)\n",
    "for layer in layers:\n",
    "    inputs.append( layer.forward_propagation(inputs[-1]))\n",
    "    print \"output shape: %s\" % (str(inputs[-1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activations = inputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.08795445e-01],\n",
       "       [  1.26133135e-06],\n",
       "       [  3.50016149e-09],\n",
       "       [  4.59924748e-11],\n",
       "       [  1.38011531e-05],\n",
       "       [  1.15288789e-10],\n",
       "       [  3.80950796e-06],\n",
       "       [  1.52790451e-08],\n",
       "       [  1.91185664e-01],\n",
       "       [  1.13188322e-12]])"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in inputs[-1]:\n",
    "    assert sum(row) - 1.0 < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(input_, targets):\n",
    "    targets = targets.T\n",
    "    assert input_.shape[0] == targets.shape[0]\n",
    "    return -np.log(np.sum(targets * activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0049455319476775228"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(activations, yt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = activations\n",
    "t = yt_batch.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print y.shape\n",
    "print t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute $\\frac{dL}{dy}$, where $y = softmax(y')$. This is what the softmax layer will propagate back to the linear layer before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "dL_dy = -t / y\n",
    "print dL_dy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\frac{dL}{dy'}$, where $y' = Vh + c$. We will be backpropagating this to the previous layer (the sigmoid). Also, while we're inside this layer, we should also compute the derivatives needed for the parameter updates: $\\frac{dL}{dV}$ and $\\frac{dL}{dc}$ (these aren't backpropagated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "dL_dyprime = y - t\n",
    "print dL_dyprime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "h = inputs[2] # h = inputs[2] = sigm(Wx+b)\n",
    "dL_dV = np.dot(dL_dyprime, h.T)\n",
    "print dL_dV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dL_dc = dL_dyprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute $\\frac{dL}{dh}$, where $h = sigmoid(h') = sigmoid(Wx + b)$. We will be backpropagating $\\frac{dL}{dh}$ to the previous (and beginning) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = layers[2].W\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dh = np.dot(V.T, dL_dyprime)\n",
    "dL_dh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, last time we do this. We have $\\frac{dL}{dh}$ from the following layer and all we need to do is compute $\\frac{dL}{dh'}$. From this, we can compute the parameter gradients $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ and we'll be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_prime = inputs[1] # h' = inputs[1] = Wx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dL_dh = dL_dh * sigmoid(h') * (1 - sigmoid(h'))\n",
    "dL_dhprime = dL_dh * h * (1 - h)\n",
    "dL_dhprime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dW = np.dot(dL_dhprime, inputs[0].T)\n",
    "dL_dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_db = dL_dhprime\n",
    "dL_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OneLayerMLP(object):\n",
    "    def __init__(self, num_input_units, num_hidden_units, num_output_units):\n",
    "        self.W = np.random.normal(0, 1, (num_hidden_units, num_input_units))\n",
    "        self.V = np.random.normal(0, 1, (num_output_units, num_hidden_units))\n",
    "        self.b = np.random.normal(0, 1, (num_hidden_units, 1))\n",
    "        self.c = np.random.normal(0, 1, (num_output_units, 1))\n",
    "        self.num_input_units = num_input_units\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_output_units = num_output_units\n",
    "    def loss(self, activation, target):\n",
    "        assert activation.shape[0] == target.shape[0]\n",
    "        return -np.log(np.sum(target * activation))\n",
    "    def classify(self, x):\n",
    "        return self.forward(x)[-1]\n",
    "    def forward(self, x, debug=False):\n",
    "        assert x.shape == (self.num_input_units, 1)\n",
    "        hprime = np.dot(self.W, x) + self.b\n",
    "        h = sigmoid(hprime)\n",
    "        yprime = np.dot(self.V, h) + self.c\n",
    "        y = softmax(yprime)\n",
    "        assert y.shape == (self.num_output_units, 1)\n",
    "        if debug:\n",
    "            print x.shape\n",
    "            print hprime.shape\n",
    "            print h.shape\n",
    "            print yprime.shape\n",
    "            print y.shape\n",
    "        return [x, hprime, h, yprime, y]\n",
    "    def backprop(self, inputs, target, learning_rate, debug=False):\n",
    "        x = inputs[0]\n",
    "        y = inputs[-1]\n",
    "        t = target\n",
    "        dL_dy = -t / y\n",
    "        dL_dyprime = y - t\n",
    "        h = inputs[2] # h = inputs[2] = sigm(Wx+b)\n",
    "        dL_dV = np.dot(dL_dyprime, h.T)\n",
    "        dL_dc = dL_dyprime\n",
    "        dL_dh = np.dot(self.V.T, dL_dyprime)\n",
    "        h_prime = inputs[1]\n",
    "        dL_dhprime = dL_dh * h * (1 - h)\n",
    "        dL_dW = np.dot(dL_dhprime, x.T)\n",
    "        dL_db = dL_dhprime\n",
    "        # test\n",
    "        if debug:\n",
    "            #print \"dl/dy\", dL_dy\n",
    "            #print \"dl/dyprime\", dL_dyprime\n",
    "            #print \"h\", h\n",
    "            #print \"dl/dv\", dL_dV\n",
    "            #print \"dl/dc\", dL_dc\n",
    "            #print \"dl/dh\", dL_dh\n",
    "            #print \"h'\", h_prime\n",
    "            #print \"dl/dhprime\", dL_dhprime\n",
    "            print \"dl/dw\", dL_dW\n",
    "            print inputs[0].T\n",
    "            #print \"dl/db\", dL_db\n",
    "            \n",
    "        assert dL_dW.shape == self.W.shape\n",
    "        assert dL_db.shape == self.b.shape\n",
    "        assert dL_dV.shape == self.V.shape\n",
    "        assert dL_dc.shape == self.c.shape\n",
    "        # ok do the parameter updates\n",
    "        self.W = self.W - learning_rate*dL_dW\n",
    "        self.b = self.b - learning_rate*dL_db\n",
    "        self.V = self.V - learning_rate*dL_dV\n",
    "        self.c = self.c - learning_rate*dL_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this loss: 0.709668\n",
      "this loss: 0.341940\n",
      "this loss: 0.276996\n",
      "this loss: 0.239796\n",
      "this loss: 0.214051\n",
      "this loss: 0.194353\n",
      "this loss: 0.178388\n",
      "this loss: 0.165016\n",
      "this loss: 0.153606\n",
      "this loss: 0.143679\n",
      "this loss: 0.134827\n",
      "this loss: 0.126813\n",
      "this loss: 0.119511\n",
      "this loss: 0.112815\n",
      "this loss: 0.106649\n",
      "this loss: 0.100952\n",
      "this loss: 0.095670\n",
      "this loss: 0.090747\n",
      "this loss: 0.086140\n",
      "this loss: 0.081819\n",
      "this loss: 0.077764\n",
      "this loss: 0.073955\n",
      "this loss: 0.070375\n",
      "this loss: 0.067004\n",
      "this loss: 0.063827\n",
      "this loss: 0.060826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c3e0f3efbbd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mthis_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"this loss: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-dc5c5d54ee7b>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, inputs, target, learning_rate, debug)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mh_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdL_dhprime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_dh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mdL_dW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dhprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdL_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_dhprime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp = OneLayerMLP(num_input_units=784, num_hidden_units=100, num_output_units=10)\n",
    "for epoch in range(0, 30):\n",
    "    this_losses = []\n",
    "    for i in range(0, Xt.shape[0]):\n",
    "        Xt_batch = Xt[i:i+1].T\n",
    "        yt_batch = one_hot( yt[i:i+1], num_classes=10 ).T\n",
    "        outputs = mlp.forward(Xt_batch, debug=False)\n",
    "        this_losses.append(mlp.loss(outputs[-1], yt_batch))\n",
    "        mlp.backprop(outputs, yt_batch, 0.01, debug=False)\n",
    "        #break\n",
    "    print \"this loss: %f\" % (np.mean(this_losses))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8a6d9a7ff3ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-dc5c5d54ee7b>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_input_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-dc5c5d54ee7b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, debug)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_input_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mhprime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhprime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.classify(Xt[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.22259175e-10],\n",
       "       [  1.12633509e-07],\n",
       "       [  3.88029628e-08],\n",
       "       [  7.61745343e-13],\n",
       "       [  2.76263221e-06],\n",
       "       [  5.38590484e-11],\n",
       "       [  9.04481362e-10],\n",
       "       [  1.54028889e-01],\n",
       "       [  1.67873714e-04],\n",
       "       [  8.45800322e-01]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = mlp.forward(Xt_batch.T, yt_batch.T)\n",
    "outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18877156],\n",
       "       [ 1.47475715],\n",
       "       [ 2.04789582],\n",
       "       [ 0.07034556],\n",
       "       [-0.17854169],\n",
       "       [ 1.01649925],\n",
       "       [-1.09767397],\n",
       "       [-1.05564543],\n",
       "       [ 2.73785476],\n",
       "       [ 2.18934815]])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.backprop(outs, yt_batch, 0.1)\n",
    "mlp.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
